{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerNet:\n",
    "    '''\n",
    "    input_size: 入力層のノード数\n",
    "    hidden_size_list: 隠れ層のノードリスト\n",
    "    output_size:　出力層のノード数\n",
    "    activation: 活性化関数\n",
    "    weight_init_std:　重みの初期化方法\n",
    "    weight_decay_lambda: 正則化の強さ\n",
    "    use_dropout: ドロップアウトの有無\n",
    "    dropout_ratio:　ドロップアウト率\n",
    "    use_batchnorm:　バッチ正規化の有無\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size_list, output_size, activation='relu', weight_init_std='relu',\n",
    "                           weight_decay_lambda=0, use_dropout_ratio = 0.5, use_batchnorm=False):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = input_size\n",
    "        self.hidden_size_list = hidden_size_list\n",
    "        self.hidden_layer_num = len(hidden_size_list)\n",
    "        self.use_dropput = use_dropout\n",
    "        self.weight_decay_lambda = weight_decay_lambda\n",
    "        self.batchnorm = use_batchnorm\n",
    "        self.params = {}\n",
    "        　\n",
    "        #　重みの初期化\n",
    "        self.__init__weight(weight_init_std)\n",
    "        \n",
    "        #　レイヤの生成\n",
    "        activation_layer = {'sigmoid': layers.Sigmoid, 'relu': layers.Relu}\n",
    "        self.layers = OrderdDICT()\n",
    "        for idx in range(1, self.hidden_layer_num+1):\n",
    "            self.layers['Affine' + str(idx)] = layers.Affine(self.params['w' + str(idx)], self.params['b' + str(idx)])\n",
    "            \n",
    "            if self.use_batchnorm:\n",
    "                self.params['gamma' + str(idx)] = np.ones(hidden_size_list[idx-1])\n",
    "                self.params['beta' + str(idx)] = np.zeros(hidden_size_list[idx-1])\n",
    "                self.layers['BatchNorm' + str(idx)] = layers.BatchNormalization(self.params['gamma' + str(idx)], self.params['beta' + str(idx)])\n",
    "                \n",
    "                self.layers['Activation_function' + str(idx)] = activation_layer[activation]()\n",
    "                \n",
    "                if self.use_dropout:\n",
    "                    self.layers['Dropout' + str(idx)] = layers.Dropout(dropout_ratio)\n",
    "        \n",
    "        idx = self.hidden_layer_num + 1\n",
    "        self.layers['Affine' + str(idx)] = layers.Affine(self.params['w' + str(idx)], self.params['b' + str(idx)])\n",
    "        \n",
    "        self.last_layer = layers.SoftmaxWithLoss()\n",
    "        \n",
    "    def __init__weight(self, weight_init_std):\n",
    "        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
    "        for idx in range(1, len(all_size_list)):\n",
    "            scale = weight_init_std\n",
    "            if str(weight_init_std).lower() in ('relu', 'he'):\n",
    "                scale = np.sqrt(2.0 / all_size_list[idx - 1]) #Heの初期値\n",
    "            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):\n",
    "                scale = np.sqrt(1.0 / all_size_list[idx - 1]) #Xavierの初期値\n",
    "            self.params['w' + str(idx)] = scale * np.random.randn(all_size_list[idx - 1], all_size_list[idx])\n",
    "            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])\n",
    "    \n",
    "    def predict(self, x, train_flg=False):\n",
    "        for key, layer in self.layers.item():\n",
    "            if 'Dropout' in key or 'BatchNorm' in key:\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, d, train_flg=False):\n",
    "        y = self.predict(x, train_flg)\n",
    "        \n",
    "        weight_decay = 0\n",
    "        for idx in range(1, self.hiddden_layer_num + 2):\n",
    "            w = self.params['w' + str(idx)]\n",
    "            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(w**2)\n",
    "            \n",
    "        return self.last_layer_forward(y, d) + weight_decay\n",
    "    \n",
    "    def accuracy(self, X, D):\n",
    "        Y = self.predict(X, train_flg=False)\n",
    "        Y = np.argmax(Y, axis=1)\n",
    "        if D.ndim ! = 1 : D = np.argmax(D, axis=1)\n",
    "            \n",
    "        accuracy = np.sum( Y == D) / float(X.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def gradient(self, X, d):\n",
    "        # forward\n",
    "        self.loss(x, d, train_flg=True)\n",
    "        \n",
    "        #backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 設定\n",
    "        grads = {}\n",
    "        for idx in range(1, self.hidden_layer_num+2):\n",
    "            grads['w' + str(idx)] = self.layers['Affine' + str(idx)].dw + self.weight_decay_lambda * self.params['w' + str(idx)]\n",
    "            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\n",
    "            \n",
    "            if self.use_batchnorm and idx != self.hidden_layer_num+1:\n",
    "                grads['gamma' + str(idx)] = self.layers['batchNorm' + str(idx)].dgamma\n",
    "                grads['beta' + str(idx)] = self.layers['BatchNorm' + str(idx)].dbeta\n",
    "                \n",
    "            return grads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
